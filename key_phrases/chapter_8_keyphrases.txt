intelligence tested: 0.4106
chatbot capable: 0.4082
humongous language: 0.2087
gap human: 0.1662
complexity tractable: 0.1574
grammars produce: 0.1501
lack reliability: 0.1422
word highest: 0.1416
response keywords: 0.1371
machine machine: 0.1355
get_vocabulary char_id: 0.1304
dog takes: 0.1262
decoder_pad_mask tf: 0.1214
suggests neural: 0.1164
gpt bert: 0.1152
kwargs assert: 0.1146
explainability extra: 0.1124
soccer does: 0.1119
srain attain: 0.1105
joking: 0.1088
william fedus: 0.1078
discuss learn: 0.1044
autos wasn: 0.1041
windows optionally: 0.1015
pass batchencoding: 0.0993
elmo contextualized: 0.099
simple rnn: 0.097
txt read_text: 0.0931
shakespeare_url: 0.087
humans counts: 0.0866
reviews extracting: 0.0858
effectiveness unsupervised: 0.0838
50 temperature: 0.083
important advances: 0.0828
seen reasonably: 0.0824
average dino: 0.0823
mitchell: 0.0774
example log_probas: 0.0759
expect list: 0.0734
image captions: 0.073
tasks simultaneously: 0.0715
53rd annual: 0.0705
deciding grant: 0.0689
architecture ignore: 0.0679
scores factor: 0.0674
contains matrix: 0.0667
sampled_softmax_loss function: 0.0666
interrogator input: 0.0662
analogous dictionary: 0.0569
bpbtsxxvpsepe check: 0.0567
tunable make: 0.0557
matmul treat: 0.0537
world natural: 0.0506
query characteristics: 0.0505
vector computationally: 0.0457
10000i evencos: 0.0454
loves indians: 0.0451
rgb color: 0.045
encodings authors: 0.0434
collapse student: 0.0429
focus values: 0.0393
open filepath: 0.0389
death sleep: 0.038
current hidden: 0.0361
rush deploy: 0.0346
24 encanta: 0.0329
seq2seq api: 0.0311
party contradicted: 0.0308
yo probability: 0.0304
classifier_mnli pipeline: 0.0303
zsl meaning: 0.0289
transformer meaning: 0.0263
dense n_tokens: 0.0257
ulmfit papers: 0.0247
just need: 0.0206
create clone: 0.0185
mapped integer: 0.0153
tensorboard callback: 0.0103
gru cells: 0.0084
france license: 0.0072
steps function: 0.0069
transpose close: 0.0061
dkeysv equation: 0.005
embed_size self: 0.0048
stateful_valid_set epochs: 0.0034
limits transfer: -0.005
datasets provides: -0.0071
conv1d layers: -0.0093
3533 2973: -0.0165
distillation means: -0.0168
electricity consumption: -0.0196
instead num_heads: -0.02
sparsecategoricalcrossentropy from_logits: -0.0211
08144 2016: -0.022
pretraining fine: -0.0257
creating hub: -0.026
notebooks videos: -0.0308
cosine frequency: -0.0316
beam width: -0.0372
woman throwing: -0.0527
